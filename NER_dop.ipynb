{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRCVSCGHARqV",
        "outputId": "51a7a5a4-0d0d-49d0-f984-288a8d6d899d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.23.5)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=7e9a99328e619ac4cc565817ac2f6671f58aeeb292b3125172933c3b2fa0f9ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=312eca34a68d01198970715baad8e07717a14fd232c5c8994ab173d5b4de4a40\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n",
            "Collecting Wikipedia-API\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2023.7.22)\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.6.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=f94d0e3af13e2f680cca5f4852930ade04ad30f744de7818dc46b3443550c447\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install natasha\n",
        "!pip install Wikipedia-API\n",
        "!pip install wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import (Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, NamesExtractor, DatesExtractor, MoneyExtractor, AddrExtractor, Doc)\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "import re"
      ],
      "metadata": {
        "id": "DzTQhO9aBj9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "data_extractor = DatesExtractor(morph_vocab)\n",
        "money_extractor = MoneyExtractor(morph_vocab)\n",
        "address_extractor = AddrExtractor(morph_vocab)"
      ],
      "metadata": {
        "id": "mbirlCpiBmtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ищет локацию, организацию или имя\n",
        "def exctract_NER(title):\n",
        "  doc=Doc(title)\n",
        "  #разбивка текста на предложения\n",
        "  doc.segment(segmenter)\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  doc.tag_ner(ner_tagger)\n",
        "\n",
        "  #for word in doc.tokens:\n",
        "  #  word.lemmatize(morph_vocab)\n",
        "\n",
        "\n",
        "  return [[x.text, x.type] for x in doc.spans]"
      ],
      "metadata": {
        "id": "mjTGZZFiTrux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find(link_dict,name,val):\n",
        "  find_dict={}\n",
        "  for id in link_dict.keys():\n",
        "    if link_dict[id][name]==val:\n",
        "      find_dict[id] = link_dict[id]\n",
        "  return find_dict\n",
        "\n",
        "def find_links(link_dict):\n",
        "  roots=find(link_dict,'rel','root')\n",
        "  id_first_root=list(roots.keys())[0]\n",
        "  #поиск связанных с рутом слов\n",
        "  predicate=find(link_dict,'head_id',id_first_root)\n",
        "  #поиск сказуемого, связанного с рутом\n",
        "  link_predicate=find(predicate,'rel','nsubj')\n",
        "  if len(list(link_predicate.keys()))==0: return {}\n",
        "  id_first_link_predicate=list(link_predicate.keys())[0]\n",
        "  return f\"{roots[id_first_root]['text']} {link_predicate[id_first_link_predicate]['text']}\"\n",
        "\n",
        "def fill_link_dict(sent):\n",
        "    link_dict={}\n",
        "    for _ in sent.syntax:\n",
        "      for word in _:\n",
        "        link_dict[word.id]={\"text\":word.text,\"head_id\":word.head_id,\"rel\":word.rel}\n",
        "    return link_dict\n",
        "\n",
        "\n",
        "def analise_links(sentances):\n",
        "  links=[]\n",
        "  for sent in sentances:\n",
        "    link_dict=fill_link_dict(sent)\n",
        "    main_link=find_links(link_dict)\n",
        "    links.append(main_link)\n",
        "  return links\n"
      ],
      "metadata": {
        "id": "n6nOBIoXn4jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ищет подлежащее и сказуемое\n",
        "def find_word_links(discription):\n",
        "  doc=Doc(discription)\n",
        "  #разбивка текста на предложения\n",
        "  doc.segment(segmenter)\n",
        "  #разбивка предложений на части речи\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  #выявления связей между словами\n",
        "  doc.parse_syntax(syntax_parser)\n",
        "  links=analise_links(doc.sents)\n",
        "  return links"
      ],
      "metadata": {
        "id": "ekwiaU_AgEsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unification_X_words(all_extracts):\n",
        "  pos_all_extracts=[x.pos for x in all_extracts]\n",
        "  idx_X_to_unif=[]\n",
        "  unif_X_words=[]\n",
        "  for i in range(len(pos_all_extracts)-1):\n",
        "    pos=pos_all_extracts[i]\n",
        "    next_pos=pos_all_extracts[i+1]\n",
        "    if pos==next_pos=='X':\n",
        "      idx_X_to_unif.append(i)\n",
        "      idx_X_to_unif.append(i+1)\n",
        "    elif (pos==\"X\"):\n",
        "      idx_X_to_unif.append(i)\n",
        "    if ('X'==pos) and (next_pos!='X'):\n",
        "      unif_X_words.append(' '.join([all_extracts[idx_X].text for idx_X in set(idx_X_to_unif)]))\n",
        "      idx_X_to_unif=[]\n",
        "  return unif_X_words"
      ],
      "metadata": {
        "id": "9Y_ScG9Dndsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#выделяет части речи\n",
        "def exctract_partOfSpeech(text, pos=[], sort=True):\n",
        "  doc=Doc(text)\n",
        "  #разбивка текста на предложения\n",
        "  doc.segment(segmenter)\n",
        "  #разбивка предложений на части речи\n",
        "  doc.tag_morph(morph_tagger)\n",
        "\n",
        "  all_extracts=[]\n",
        "  sort_extracts=[]\n",
        "  for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "    if (token.pos in pos):\n",
        "       if not sort: print(token.lemma, token.pos)\n",
        "       sort_extracts.append(token.lemma)\n",
        "\n",
        "    all_extracts.append(token)\n",
        "  if not sort:\n",
        "    return all_extracts\n",
        "  if 'X' in pos:\n",
        "    #return sort_extracts\n",
        "    return unification_X_words(all_extracts)\n",
        "\n",
        "  return sort_extracts"
      ],
      "metadata": {
        "id": "deh5MDiVBecd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def request_wiki(name,language):\n",
        "  wiki_wiki = wikipediaapi.Wikipedia('reqests (reqest@example.com)',language)\n",
        "  #если ни один X_word не был объединен с другим\n",
        "  if len(name)<2:\n",
        "    return 0\n",
        "\n",
        "  #Если название чего-либо было случайно обрезано, то на том шаге исчитаться к какому названию в ввикипедии ближе всего и замаеняется\n",
        "  page = wiki_wiki.page(name)\n",
        "\n",
        "  #Проверка существует ли страница в вики\n",
        "  if not page.exists():\n",
        "    return 0\n",
        "  return page\n",
        "\n",
        "def prepare_wiki_requst(name):\n",
        "  try:\n",
        "    name=wikipedia.search(name)[0]\n",
        "  except:\n",
        "    pass\n",
        "  page_en=request_wiki(name,'en')\n",
        "  page_ru=request_wiki(name,'ru')\n",
        "\n",
        "\n",
        "  analise_dict={}\n",
        "  if (page_en==0) or (page_ru==0): return analise_dict\n",
        "\n",
        "  analise_dict['category']=list(page_en.categories.keys())[0]#[:9]\n",
        "  #analise_dict['summary']=page_ru.summary\n",
        "  analise_dict['NER']=exctract_NER(page_ru.summary)\n",
        "\n",
        "  return analise_dict"
      ],
      "metadata": {
        "id": "pQ4fPCR1QWTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_text(text):\n",
        " cleaned_text = re.sub(r'[^-а-яА-Яa-zA-Z.,/ ]', '', re.sub(r'\\([^)]*\\)|\\{[^}]*\\}|\\[[^\\]]*\\]', '', text))\n",
        " return cleaned_text"
      ],
      "metadata": {
        "id": "zuSxgiFd-qwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_analise(title,discription):\n",
        "  analise_dict={}\n",
        "  title=clear_text(title)\n",
        "  #выделяються собственные существительные\n",
        "  analise_dict['names']=exctract_partOfSpeech(title, ['PROPN'])\n",
        "\n",
        "  #выделение всех существительных\n",
        "  analise_dict['NOUN']=exctract_partOfSpeech(title, ['NOUN'])\n",
        "\n",
        "  #выделение NER\n",
        "  analise_dict['NER']=exctract_NER(title)\n",
        "\n",
        "  #поиск слов в вики, которые не удалось классифицировать как части речи\n",
        "  wiki={}\n",
        "  dont_find_words=exctract_partOfSpeech(title, ['X'])\n",
        "  for dfw in dont_find_words:\n",
        "    wiki[dfw]=prepare_wiki_requst(dfw)\n",
        "\n",
        "  analise_dict['wiki']=wiki\n",
        "\n",
        "  return analise_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "3a7Mg8VsXPz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title=\"\"\"Присутствует спойлеры и ненормативная лексика. В этой нарезке вы увидите: Как использовать паучье чутье в Spider-Man, косплей на человека-паука, мнение об игре и другие забавные моменты\"\"\"\n",
        "#title=\"Прохождение Red Dead Redemption 2 | RDR2 | TheHedwigGames\"\n",
        "discription=\"Прохождение Red Dead Redemption 2 | RDR2 | TheHedwigGames\""
      ],
      "metadata": {
        "id": "ecyh0qY6GgvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_analise(title,discription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gUTKWdsdLa3",
        "outputId": "6402b469-b45e-4f9f-923e-1dfb9eec58aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object exctract_NER.<locals>.<genexpr> at 0x7d2c3a50eea0>\n",
            "<generator object exctract_NER.<locals>.<genexpr> at 0x7d2c3a50e880>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'names': [],\n",
              " 'NOUN': ['спойлер',\n",
              "  'лексика',\n",
              "  'нарезка',\n",
              "  'чутье',\n",
              "  'человек-паук',\n",
              "  'мнение',\n",
              "  'игра',\n",
              "  'момент'],\n",
              " 'NER': [['Spider-Man', 'ORG']],\n",
              " 'wiki': {'Spider-Man': {'category': 'Category:1962 comics debuts',\n",
              "   'NER': [['Пи́тер Бе́нджамин Па́ркер', 'PER'],\n",
              "    ['Peter Benjamin Parker)', 'PER'],\n",
              "    ['Marvel Comics', 'ORG'],\n",
              "    ['Стэном Ли', 'PER'],\n",
              "    ['Стивом Дитко', 'PER'],\n",
              "    ['Ли', 'PER'],\n",
              "    ['Дитко', 'PER'],\n",
              "    ['Стив Роджерс', 'PER'],\n",
              "    ['Робина', 'PER'],\n",
              "    ['Бена', 'PER'],\n",
              "    ['Marvel', 'ORG'],\n",
              "    ['Питер Паркер', 'PER'],\n",
              "    ['Питера Паркера', 'PER'],\n",
              "    ['Человек-паук', 'PER'],\n",
              "    ['Тоби Магуайр', 'PER'],\n",
              "    ['Сэма Рэйми', 'PER'],\n",
              "    ['Эндрю Гарфилд', 'PER'],\n",
              "    ['Марка Уэбба', 'PER'],\n",
              "    ['Том Холланд', 'PER'],\n",
              "    ['Marvel', 'ORG'],\n",
              "    ['Рив Карни', 'PER'],\n",
              "    ['Питера Паркера', 'PER']]}}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0De9coR72zfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}